{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_CONLL_NER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNKhShOaylVt",
        "colab_type": "code",
        "outputId": "ea53c887-1f8e-4a47-ecf0-606645cf5679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "#Installing required packages \n",
        "!pip install pytorch-pretrained-bert==0.4.0\n",
        "!pip install seqeval==0.0.12\n",
        "\n",
        "#importing packages for string processing,dataframe handling, array manipulations, etc\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "#importing all the pytorch packages\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
        "\n",
        "#importing additonal packages to aid preprocessing of data\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#importing packages to calculate the f1_score of our model\n",
        "from seqeval.metrics import f1_score\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert==0.4.0 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (1.10.47)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (1.17.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.4.0) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.4.0) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.4.0) (1.13.47)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.4.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.4.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert==0.4.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert==0.4.0) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert==0.4.0) (1.12.0)\n",
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.12) (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.12) (1.17.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12) (1.12.0)\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVfxrl-HOkTn",
        "colab_type": "code",
        "outputId": "18e81a2a-c6cc-463e-863d-79864179a4fb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "#uploading data into google colab\n",
        "#upload the test.txt and train.txt files respectively\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-677b6807-f81e-43bf-b089-e78440e7bc45\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-677b6807-f81e-43bf-b089-e78440e7bc45\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.txt to test (2).txt\n",
            "Saving train.txt to train (2).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkbURKk5y0_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Load the training/testing data. \n",
        "input: conll format data, but with only 2 tab separated colums - words and NEtags.\n",
        "output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n",
        "\"\"\"\n",
        "#functions for preparing the data in the *.txt files\n",
        "def load__data_conll(file_path):\n",
        "    myoutput,words,tags = [],[],[]\n",
        "    fh = open(file_path)\n",
        "    for line in fh:\n",
        "        line = line.strip()\n",
        "        if \"\\t\" not in line:\n",
        "            #Sentence ended.\n",
        "            myoutput.append([words,tags])\n",
        "            words,tags = [],[]\n",
        "        else:\n",
        "            word, tag = line.split(\"\\t\")\n",
        "            words.append(word)\n",
        "            tags.append(tag)\n",
        "    fh.close()\n",
        "    return myoutput\n",
        "\n",
        "\"\"\"\n",
        "Get features for all words in the sentence\n",
        "Features:\n",
        "- word context: a window of 2 words on either side of the current word, and current word.\n",
        "- POS context: a window of 2 POS tags on either side of the current word, and current tag. \n",
        "input: sentence as a list of tokens.\n",
        "output: list of dictionaries. each dict represents features for that word.\n",
        "\"\"\"\n",
        "def sent2feats(sentence):\n",
        "    feats = []\n",
        "    sen_tags = pos_tag(sentence) #This format is specific to this POS tagger!\n",
        "    for i in range(0,len(sentence)):\n",
        "        word = sentence[i]\n",
        "        wordfeats = {}\n",
        "       #word features: word, prev 2 words, next 2 words in the sentence.\n",
        "        wordfeats['word'] = word\n",
        "        if i == 0:\n",
        "            wordfeats[\"prevWord\"] = wordfeats[\"prevSecondWord\"] = \"<S>\"\n",
        "        elif i==1:\n",
        "            wordfeats[\"prevWord\"] = sentence[0]\n",
        "            wordfeats[\"prevSecondWord\"] = \"</S>\"\n",
        "        else:\n",
        "            wordfeats[\"prevWord\"] = sentence[i-1]\n",
        "            wordfeats[\"prevSecondWord\"] = sentence[i-2]\n",
        "        #next two words as features\n",
        "        if i == len(sentence)-2:\n",
        "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
        "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
        "        elif i==len(sentence)-1:\n",
        "            wordfeats[\"nextWord\"] = \"</S>\"\n",
        "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
        "        else:\n",
        "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
        "            wordfeats[\"nextNextWord\"] = sentence[i+2]\n",
        "        \n",
        "        #POS tag features: current tag, previous and next 2 tags.\n",
        "        wordfeats['tag'] = sen_tags[i][1]\n",
        "        if i == 0:\n",
        "            wordfeats[\"prevTag\"] = wordfeats[\"prevSecondTag\"] = \"<S>\"\n",
        "        elif i == 1:\n",
        "            wordfeats[\"prevTag\"] = sen_tags[0][1]\n",
        "            wordfeats[\"prevSecondTag\"] = \"</S>\"\n",
        "        else:\n",
        "            wordfeats[\"prevTag\"] = sen_tags[i - 1][1]\n",
        "\n",
        "            wordfeats[\"prevSecondTag\"] = sen_tags[i - 2][1]\n",
        "            # next two words as features\n",
        "        if i == len(sentence) - 2:\n",
        "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
        "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
        "        elif i == len(sentence) - 1:\n",
        "            wordfeats[\"nextTag\"] = \"</S>\"\n",
        "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
        "        else:\n",
        "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
        "            wordfeats[\"nextNextTag\"] = sen_tags[i + 2][1]\n",
        "        #That is it! You can add whatever you want!\n",
        "        feats.append(wordfeats)\n",
        "    return feats\n",
        "\n",
        "#preprocess the data by calling the functions\n",
        "train_path = 'train.txt'\n",
        "test_path = 'test.txt'\n",
        "conll_train = load__data_conll(train_path)\n",
        "conll_test = load__data_conll(test_path)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYm4B66pBN6T",
        "colab_type": "code",
        "outputId": "834bbc7f-b531-4792-c967-4486d5391684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#BERT needs us to pre-process the data in a particular way.\n",
        "#Lets take the raw data from the txt files\n",
        "df_train = pd.read_csv(\"train.txt\", engine=\"python\",delimiter=\"\\t\",header=None)\n",
        "df_test = pd.read_csv(\"test.txt\", engine=\"python\",delimiter=\"\\t\",encoding='utf-8',header=None, error_bad_lines=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 23407: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQS3WdwLBOJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#merge \n",
        "df = pd.merge(df_train,df_test)\n",
        "label = list(df[1].values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7yRSvrX4-z",
        "colab_type": "code",
        "outputId": "93f4e49d-b54f-4ed5-ce78-8401c677a182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "np.array(conll_train).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14041, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BGxbUspYAPU",
        "colab_type": "code",
        "outputId": "77637d72-af32-476d-e22e-a34218e3d985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "np.array(conll_test).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3453, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEM8-XuPYeWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_pp = pd.DataFrame(conll_train,columns=[\"sentence\",\"labels\"])\n",
        "df_test_pp = pd.DataFrame(conll_test,columns=[\"sentence\",\"labels\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WgHCbX_ZAqG",
        "colab_type": "code",
        "outputId": "9c051555-125a-407a-a29a-92a022943887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "all_sentences = list(df_train_pp['sentence'])+list(df_test_pp['sentence'])\n",
        "print(len(all_sentences))\n",
        "all_labels = list(df_train_pp['labels'])+list(df_test_pp['labels']) \n",
        "print(len(all_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17494\n",
            "17494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KW3zKy7ZAnB",
        "colab_type": "code",
        "outputId": "ba8d593d-495f-4711-82c0-13d28b06d164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "all_sentences[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h25ZOnmyyAhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#renaming the above \n",
        "sentences = all_sentences\n",
        "labels = all_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JUPhdRFzea1",
        "colab_type": "code",
        "outputId": "dd596a70-3aea-4c78-fc8b-f617b0eab5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#setting up pytorch to use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "#prescribed configurations that we need to fix for BERT.\n",
        "MAX_LEN = 75\n",
        "bs = 32\n",
        "\n",
        "#BERT's implementation comes with a pretained tokenizer and a defined vocabulary\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "#tokenizing the text\n",
        "tokenized_texts = [[tokenizer.tokenize(sent) for sent in i] for i in sentences]\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['eu'], ['rejects'], ['german'], ['call'], ['to'], ['boycott'], ['british'], ['lamb'], ['.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XArUfLNp5bbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#flattening the list inside\n",
        "c=0\n",
        "for i in tokenized_texts:\n",
        "  res=[]\n",
        "  for j in i:\n",
        "    res += j\n",
        "  tokenized_texts[c] = res\n",
        "  c +=1\n",
        "\n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHnClTR15ArX",
        "colab_type": "code",
        "outputId": "817899c5-4959-4844-e0f8-772a8ccc0c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tokenized_texts[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['peter', 'blackburn']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niy1e_tU7F0c",
        "colab_type": "code",
        "outputId": "9578e4d6-625b-41ab-89d8-2beb89191aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "labels[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-PER', 'I-PER']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xxx7mF28NZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pre-processing the labels\n",
        "tags_vals = list(set(label))\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvUnUjvk8sOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cutting and padding the tokens and labels to our desired length\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDb-q3HXEumu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BERT supports something called attention masks\n",
        "#Tells the model which tokens should be attended to, and which should not.\n",
        "#learn more about this at https://huggingface.co/transformers/glossary.html#attention-mask\n",
        "\n",
        "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LuJ9LeDEwex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split the dataset to use 10% to validate the model.\n",
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuS8WeQyEyG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch requires inouts to be in the form of torch tensors\n",
        "#Learn more about torch tensors at https://pytorch.org/docs/stable/tensors.html\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-YtGlTXEztA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the Data Loaders\n",
        "#Shuffle the data at training time\n",
        "#Pass them sequentially during test time\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajuboku7E1Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fine-Tuning BERT\n",
        "# BertForTokenClassification class of pytorch-pretrained-bert package provides  for token-level predictions\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En7S5Jj2E3Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Passing model parameters into GPU\n",
        "model.cuda();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it5l2g-cE439",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Before starting fine tuing we need to add the optimizer. Generally Adam is used\n",
        "#weight_decay is added as regularization to the main weight matrices\n",
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7jNHHs1E6u1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#accuracy\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZHdynvRFDv1",
        "colab_type": "code",
        "outputId": "a6aea537-c751-4f5c-9236-84daf4f433a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "#Add the epoch number. The paper recomends 3-4\n",
        "#Link to the paper https://arxiv.org/abs/1810.04805\n",
        "epochs = 5\n",
        "max_grad_norm = 1.0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    # VALIDATION on validation set\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask, labels=b_labels)\n",
        "            logits = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        \n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
        "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
        "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.10508737136132834\n",
            "Validation loss: 0.05539024065841328\n",
            "Validation Accuracy: 0.9812424242424242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 1/5 [04:44<18:58, 284.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score: 0.5446395218528203\n",
            "Train loss: 0.047229099411108505\n",
            "Validation loss: 0.03818079040131785\n",
            "Validation Accuracy: 0.9866088154269971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [09:40<14:24, 288.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score: 0.6793060275442676\n",
            "Train loss: 0.031095164933060183\n",
            "Validation loss: 0.03134444153613665\n",
            "Validation Accuracy: 0.9897286501377408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [14:37<09:41, 290.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score: 0.7575379125780554\n",
            "Train loss: 0.024061746311816956\n",
            "Validation loss: 0.028725732481953772\n",
            "Validation Accuracy: 0.9908712121212119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [19:33<04:52, 292.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score: 0.788881069669247\n",
            "Train loss: 0.01966510504310027\n",
            "Validation loss: 0.027338437329639087\n",
            "Validation Accuracy: 0.9912913223140495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch: 100%|██████████| 5/5 [24:29<00:00, 293.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score: 0.7988942639944714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqic4K83FLKX",
        "colab_type": "code",
        "outputId": "e7ee45bc-84b4-4b92-b05f-8c852fc7fd72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#Evaluate the model\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "for batch in valid_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                              attention_mask=b_input_mask, labels=b_labels)\n",
        "        logits = model(b_input_ids, token_type_ids=None,\n",
        "                       attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    true_labels.append(label_ids)\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    nb_eval_examples += b_input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
        "valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
        "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
        "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.027338437329639087\n",
            "Validation Accuracy: 0.9912913223140495\n",
            "Validation F1-Score: 0.7988942639944714\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}